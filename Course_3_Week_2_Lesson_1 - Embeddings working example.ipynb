{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course 3 - Week 2 - Lesson 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-AhVYeBWgQ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "93d648fb-836d-4d17-eb94-1596aa9ff08b"
      },
      "source": [
        "# NOTE: PLEASE MAKE SURE YOU ARE RUNNING THIS IN A PYTHON3 ENVIRONMENT\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "# This is needed for the iterator over the data\n",
        "# But not necessary if you have TF 2.0 installed\n",
        "# !pip install tensorflow==2.0.0-beta0\n",
        "\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "!pip install -q tensorflow-datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E0V5MHp8CTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow as tf\n",
        "# tf.enable_eager_execution()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IoM4VFxWpMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiuvsUmT63oB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "a2961334-837e-40b5-c3c0-b4d0eac7f756"
      },
      "source": [
        "info"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='imdb_reviews',\n",
              "    version=0.1.0,\n",
              "    description='Large Movie Review Dataset.\n",
              "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
              "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
              "    features=FeaturesDict({\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
              "        'text': Text(shape=(), dtype=tf.string),\n",
              "    }),\n",
              "    total_num_examples=100000,\n",
              "    splits={\n",
              "        'test': 25000,\n",
              "        'train': 25000,\n",
              "        'unsupervised': 50000,\n",
              "    },\n",
              "    supervised_keys=('text', 'label'),\n",
              "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
              "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
              "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
              "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
              "      month     = {June},\n",
              "      year      = {2011},\n",
              "      address   = {Portland, Oregon, USA},\n",
              "      publisher = {Association for Computational Linguistics},\n",
              "      pages     = {142--150},\n",
              "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
              "    }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHQ2Ko0zl7M4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train_data, test_data = imdb['train'], imdb['test']\n",
        "\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "\n",
        "testing_sentences = []\n",
        "testing_labels = []\n",
        "\n",
        "# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\n",
        "for s,l in train_data:\n",
        "  training_sentences.append(str(s.numpy()))\n",
        "  training_labels.append(l.numpy())\n",
        "  \n",
        "for s,l in test_data:\n",
        "  testing_sentences.append(str(s.numpy()))\n",
        "  testing_labels.append(l.numpy())\n",
        "  \n",
        "training_labels_final = np.array(training_labels)\n",
        "testing_labels_final = np.array(testing_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgE-awFP6mZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "19c52a3a-b3a1-48aa-fd1a-145dbe843e02"
      },
      "source": [
        "# for s,l in train_data:\n",
        "#   j=s\n",
        "\n",
        "j.numpy()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b\"Late one night on a desolate road, in an empty saloon Martin Sheen spins a yarn for Robert Carradine of Hopalong Cassidy and friends tracking a group of murderous cattle rustlers, who've killed a few men and kidnapped Cassidy's girl.<br /><br />Writer/director Christopher Coppola May have incurred the wrath of William Boyd purists by daring to make a modern low budget film featuring their beloved Hoppy, but I'm glad he did it! No character should be so tied to an an actor that no one else ever be allowed to play him or her again!<br /><br />I thought it was good fun and an interesting updating of the classic programmers of the thirties and forties. Though guilty of some bad acting, this is earnest enough and unpretentious, making it hard for me to dislike.<br /><br />The whole production is a bit odd though, but I really enjoyed the scenes between Sheen and Carridine. The fact that we're watching a story within a story makes the oddness and exaggerations more palatable.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93LrXiol9Yaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# numpy() extracts the data from the tensor \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n15yyMdmoH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 16\n",
        "max_length = 120\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded = pad_sequences(sequences,maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9axf0uIXVMhO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "5670ee52-f9c3-4a40-b073-cd5db6379c29"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(decode_review(padded[1]))\n",
        "print(training_sentences[1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b oh yeah jenna jameson did it again yeah baby this movie rocks it was one of the 1st movies i saw of her and i have to say i feel in love with her she was great in this move br br her performance was outstanding and what i liked the most was the scenery and the wardrobe it was amazing you can tell that they put a lot into the movie the girls cloth were amazing br br i hope this comment helps and u can buy the movie the storyline is awesome is very unique and i'm sure u are going to like it jenna amazed us once more and no wonder the movie won so many\n",
            "b\"Oh yeah! Jenna Jameson did it again! Yeah Baby! This movie rocks. It was one of the 1st movies i saw of her. And i have to say i feel in love with her, she was great in this move.<br /><br />Her performance was outstanding and what i liked the most was the scenery and the wardrobe it was amazing you can tell that they put a lot into the movie the girls cloth were amazing.<br /><br />I hope this comment helps and u can buy the movie, the storyline is awesome is very unique and i'm sure u are going to like it. Jenna amazed us once more and no wonder the movie won so many awards. Her make-up and wardrobe is very very sexy and the girls on girls scene is amazing. specially the one where she looks like an angel. It's a must see and i hope u share my interests\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NEpdhb8AxID",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "6e224ca5-0ee0-4dd9-d418-6b9fbebff03f"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 120, 16)           160000    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1920)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 11526     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 171,533\n",
            "Trainable params: 171,533\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5LLrXC-uNX6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "3f3fe037-f17b-4856-9408-5d5055684d54"
      },
      "source": [
        "num_epochs = 10\n",
        "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 10s 382us/sample - loss: 0.4957 - acc: 0.7404 - val_loss: 0.3477 - val_acc: 0.8454\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 7s 286us/sample - loss: 0.2378 - acc: 0.9077 - val_loss: 0.3722 - val_acc: 0.8374\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 7s 285us/sample - loss: 0.0898 - acc: 0.9777 - val_loss: 0.4618 - val_acc: 0.8220\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 7s 280us/sample - loss: 0.0223 - acc: 0.9972 - val_loss: 0.5287 - val_acc: 0.8248\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 7s 280us/sample - loss: 0.0055 - acc: 0.9996 - val_loss: 0.5956 - val_acc: 0.8215\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 7s 284us/sample - loss: 0.0018 - acc: 1.0000 - val_loss: 0.6505 - val_acc: 0.8233\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 7s 283us/sample - loss: 8.1743e-04 - acc: 1.0000 - val_loss: 0.6823 - val_acc: 0.8252\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 7s 283us/sample - loss: 4.5684e-04 - acc: 1.0000 - val_loss: 0.7228 - val_acc: 0.8256\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 7s 282us/sample - loss: 2.6318e-04 - acc: 1.0000 - val_loss: 0.7568 - val_acc: 0.8258\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 7s 288us/sample - loss: 1.5811e-04 - acc: 1.0000 - val_loss: 0.7937 - val_acc: 0.8258\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3b7b9b2128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnWCeeKmBOgf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "dc5ff682-981c-4312-e38a-431688fdec5f"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(6, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "num_epochs = 10\n",
        "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 120, 16)           160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d (Gl (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 102       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 160,109\n",
            "Trainable params: 160,109\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 8s 312us/sample - loss: 0.5510 - acc: 0.7446 - val_loss: 0.3856 - val_acc: 0.8418\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 7s 291us/sample - loss: 0.3330 - acc: 0.8627 - val_loss: 0.3347 - val_acc: 0.8554\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 7s 291us/sample - loss: 0.2709 - acc: 0.8915 - val_loss: 0.3337 - val_acc: 0.8558\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 7s 286us/sample - loss: 0.2354 - acc: 0.9094 - val_loss: 0.3435 - val_acc: 0.8538\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 7s 290us/sample - loss: 0.2092 - acc: 0.9212 - val_loss: 0.3631 - val_acc: 0.8478\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 7s 290us/sample - loss: 0.1890 - acc: 0.9314 - val_loss: 0.3898 - val_acc: 0.8411\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 7s 292us/sample - loss: 0.1710 - acc: 0.9385 - val_loss: 0.4190 - val_acc: 0.8363\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 7s 292us/sample - loss: 0.1568 - acc: 0.9463 - val_loss: 0.4466 - val_acc: 0.8328\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 7s 292us/sample - loss: 0.1433 - acc: 0.9517 - val_loss: 0.4845 - val_acc: 0.8283\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 7s 292us/sample - loss: 0.1317 - acc: 0.9570 - val_loss: 0.5219 - val_acc: 0.8222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3b7b4592e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAmjJqEyCOF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdabe513-c31b-46f8-9e5a-57141e09d3f0"
      },
      "source": [
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmB0Uxk0ycP6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "for word_num in range(1, vocab_size):\n",
        "  word = reverse_word_index[word_num]\n",
        "  embeddings = weights[word_num]\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDeqpOCVydtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "try:\n",
        "  from google.colab import files\n",
        "except ImportError:\n",
        "  pass\n",
        "else:\n",
        "  files.download('vecs.tsv')\n",
        "  files.download('meta.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRxoxc2apscY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b14a8fec-bf7d-4efa-f688-786657fedbc8"
      },
      "source": [
        "sentence = \"I really think this is amazing. honest.\"\n",
        "sequence = tokenizer.texts_to_sequences(sentence)\n",
        "print(sequence)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[11], [], [1430], [968], [4], [1537], [1537], [4738], [], [790], [2015], [11], [2922], [2191], [], [790], [2015], [11], [579], [], [11], [579], [], [4], [1783], [4], [4508], [11], [2922], [1277], [], [], [2015], [1005], [2922], [968], [579], [790], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSFAmJPvA3ee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}